    async def prepare_features(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Dict[str, np.ndarray]:
        """
        Prepare all features for model training/prediction.
        
        Args:
            start_time: Start time for feature calculation
            end_time: End time for feature calculation
            
        Returns:
            Dictionary of normalized features grouped by type
        """
        try:
            # 1. Collect base price data
            df = await self._collect_price_data(start_time, end_time)
            
            # 2. Add technical indicators
            df = self.technical_indicators.calculate_all(df)
            
            # 3. Add market depth features
            depth_features = await self._compute_market_depth_features(start_time, end_time)
            df = df.join(depth_features)
            
            # 4. Add derivatives market features
            derivatives_features = await self._compute_derivatives_features(start_time, end_time)
            df = df.join(derivatives_features)
            
            # 5. Add sentiment features
            sentiment_features = await self._compute_sentiment_features(start_time, end_time)
            df = df.join(sentiment_features)
            
            # 6. Add correlated assets features
            correlated_features = await self._compute_correlated_assets_features(start_time, end_time)
            df = df.join(correlated_features)
            
            # 7. Normalize features by group
            normalized_features = {}
            for group, feature_list in self.config.FEATURE_GROUPS.items():
                if all(feature in df.columns for feature in feature_list):
                    group_data = df[feature_list]
                    normalized = self.scalers[group].fit_transform(group_data)
                    normalized_features[group] = normalized
            
            logger.info("Features prepared successfully")
            return normalized_features
            
        except Exception as e:
            logger.error(f"Failed to prepare features: {e}")
            raise

    async def _collect_price_data(self, start_time: pd.Timestamp,
                                end_time: pd.Timestamp) -> pd.DataFrame:
        """Collect OHLCV data for the main trading pair."""
        try:
            df = await self.market_data.fetch_ohlcv(
                symbol=self.config.PRICE_SYMBOL,
                timeframe=f"{self.config.DATA_INTERVAL_MINUTES}m",
                start_time=start_time,
                end_time=end_time
            )
            logger.debug(f"Collected price data: {len(df)} rows")
            return df
        except Exception as e:
            logger.error(f"Failed to collect price data: {e}")
            raise

    async def _compute_market_depth_features(self, start_time: pd.Timestamp,
                                           end_time: pd.Timestamp) -> pd.DataFrame:
        """Compute order book depth and imbalance features."""
        try:
            features = []
            current_time = start_time
            
            while current_time <= end_time:
                depth_data = await self.market_data.fetch_order_book(
                    symbol=self.config.PRICE_SYMBOL,
                    limit=self.config.MARKET_DEPTH_LEVELS
                )
                
                features.append({
                    'timestamp': current_time,
                    'bid_sum': depth_data['bid_sum'],
                    'ask_sum': depth_data['ask_sum'],
                    'imbalance': depth_data['imbalance'],
                    'spread': (depth_data['asks'][0][0] - depth_data['bids'][0][0]) / depth_data['bids'][0][0]
                })
                
                current_time += pd.Timedelta(minutes=self.config.DATA_INTERVAL_MINUTES)
            
            df = pd.DataFrame(features).set_index('timestamp')
            logger.debug(f"Computed market depth features: {len(df)} rows")
            return df
            
        except Exception as e:
            logger.error(f"Failed to compute market depth features: {e}")
            raise

    async def _compute_derivatives_features(self, start_time: pd.Timestamp,
                                         end_time: pd.Timestamp) -> pd.DataFrame:
        """Compute features from derivatives market data."""
        try:
            features = []
            current_time = start_time
            
            while current_time <= end_time:
                funding_rate = await self.market_data.fetch_funding_rate(
                    symbol=self.config.PRICE_SYMBOL
                )
                open_interest = await self.market_data.fetch_open_interest(
                    symbol=self.config.PRICE_SYMBOL
                )
                
                features.append({
                    'timestamp': current_time,
                    'funding_rate': funding_rate,
                    'open_interest': open_interest,
                    'oi_change': 0  # Will be calculated after creating DataFrame
                })
                
                current_time += pd.Timedelta(minutes=self.config.DATA_INTERVAL_MINUTES)
            
            df = pd.DataFrame(features).set_index('timestamp')
            df['oi_change'] = df['open_interest'].pct_change()
            
            logger.debug(f"Computed derivatives features: {len(df)} rows")
            return df
            
        except Exception as e:
            logger.error(f"Failed to compute derivatives features: {e}")
            raise

    async def _compute_sentiment_features(self, start_time: pd.Timestamp,
                                       end_time: pd.Timestamp) -> pd.DataFrame:
        """Compute sentiment features from news and social media."""
        try:
            # Fetch sentiment data
            news_sentiment = await self.sentiment_collector.fetch_news_sentiment(
                days=(end_time - start_time).days + 1
            )
            social_sentiment = await self.sentiment_collector.fetch_social_sentiment()
            
            # Aggregate sentiment
            sentiment_df = self.sentiment_collector.aggregate_sentiment(
                news_sentiment,
                social_sentiment
            )
            
            # Resample to match data interval if needed
            if self.config.DATA_INTERVAL_MINUTES != 60:
                sentiment_df = sentiment_df.resample(
                    f"{self.config.DATA_INTERVAL_MINUTES}T"
                ).ffill()
            
            logger.debug(f"Computed sentiment features: {len(sentiment_df)} rows")
            return sentiment_df
            
        except Exception as e:
            logger.error(f"Failed to compute sentiment features: {e}")
            raise

    async def _compute_correlated_assets_features(self, start_time: pd.Timestamp,
                                                end_time: pd.Timestamp) -> pd.DataFrame:
        """Compute features from correlated assets."""
        try:
            dfs = []
            
            for symbol in self.config.CORRELATED_PAIRS:
                df = await self.market_data.fetch_ohlcv(
                    symbol=symbol,
                    timeframe=f"{self.config.DATA_INTERVAL_MINUTES}m",
                    start_time=start_time,
                    end_time=end_time
                )
                
                # Compute correlation features
                symbol_prefix = symbol.replace('/', '_')
                df[f'{symbol_prefix}_returns'] = df['close'].pct_change()
                df[f'{symbol_prefix}_volatility'] = df['close'].pct_change().rolling(window=30).std()
                df[f'{symbol_prefix}_correlation'] = (
                    df['close'].rolling(window=30)
                    .corr(df['volume'])
                )
                
                # Keep only the features we want
                feature_cols = [f'{symbol_prefix}_{suffix}' for suffix in ['returns', 'volatility', 'correlation']]
                dfs.append(df[feature_cols])
            
            result = pd.concat(dfs, axis=1)
            logger.debug(f"Computed correlated assets features: {len(result)} rows")
            return result
            
        except Exception as e:
            logger.error(f"Failed to compute correlated assets features: {e}")
            raise